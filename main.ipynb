{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "model=\"vertex_ai/gemini-2.0-flash-thinking-exp-01-21\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import docx\n",
    "import pandas as pd\n",
    "\n",
    "def read_file_content(filepath):\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(filepath):\n",
    "        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n",
    "    \n",
    "    # Get the file extension\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "    file_extension = file_extension.lower()\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        return read_pdf(filepath)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx(filepath)\n",
    "    elif file_extension in ['.xlsx', '.csv']:\n",
    "        return read_spreadsheet(filepath)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Allowed types are: .pdf, .docx, .xlsx, .csv\")\n",
    "\n",
    "def read_pdf(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        pages_to_read = num_pages // 2  # Read first 50% of pages\n",
    "        content = []\n",
    "        for page_num in range(pages_to_read):\n",
    "            page = reader.pages[page_num]\n",
    "            content.append(page.extract_text())\n",
    "    return '\\n'.join(content)\n",
    "\n",
    "def read_docx(filepath):\n",
    "    doc = docx.Document(filepath)\n",
    "    paragraphs = doc.paragraphs\n",
    "    num_paragraphs = len(paragraphs)\n",
    "    paragraphs_to_read = num_paragraphs // 2  # Read first 50% of paragraphs\n",
    "    content = [paragraphs[i].text for i in range(paragraphs_to_read)]\n",
    "    return '\\n'.join(content)\n",
    "\n",
    "def read_spreadsheet(filepath):\n",
    "    if filepath.endswith('.csv'):\n",
    "        df = pd.read_csv(filepath)\n",
    "    else:\n",
    "        df = pd.read_excel(filepath)\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    rows_to_read = num_rows // 5  # Calculate 20% of the total rows\n",
    "    df_subset = df.head(rows_to_read)  # Select the first 20% of rows\n",
    "    \n",
    "    # Convert the DataFrame subset to a string\n",
    "    df_string = df_subset.to_string(index=False)\n",
    "    \n",
    "    return df_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# content = read_file_content('data.csv')\n",
    "# print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatVertexAI(model_name= \"gemini-1.5-flash-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "content1 = read_file_content(\"FAhad Resume.pdf\")\n",
    "content2 = read_file_content(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_selection_agent = Agent(\n",
    "    llm=llm,\n",
    "    role=\"File Selection Expert\",\n",
    "    goal=\"Analyze user queries and return relevant file names based on file content summaries.\",\n",
    "    backstory=\"A highly skilled data indexing expert who can efficiently match user needs with available document summaries.\",\n",
    "    allow_code_execution=False  # No need for code execution, just text processing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_matching_task = Task(\n",
    "    description=\"Compare the user query with file content summaries and return a list of relevant file names that can be used for the user query. The file summaries are: {file_summaries}\\n The user query is: {user_query}\",\n",
    "    agent=file_selection_agent,\n",
    "    expected_output=\"A list of filenames that match the user query.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 22:42:33,911 - 35236 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "file_selection_crew = Crew(\n",
    "    agents=[file_selection_agent],\n",
    "    tasks=[file_matching_task],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_summary = model.invoke(file_summary_generation_prompt(content1))\n",
    "file2_summary = model.invoke(file_summary_generation_prompt(content2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_summary_dict = {'FAhad Resume.pdf': file1_summary.content, \"data.csv\": file2_summary.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAhad Resume.pdf': 'This is a resume for Fahad Ul Rehman, an AI engineer with 5 years of experience. He specializes in generative AI projects and prompt engineering, with proficiency in FastAPI, MongoDB, and cloud-based AI services. He has worked on various AI applications, including procurement and financial data analysis, computer vision, and natural language processing, demonstrating his skills in machine learning, data analytics, and model development. The document showcases his expertise and career history, highlighting his capabilities and achievements in the field of artificial intelligence. \\n',\n",
       " 'data.csv': 'This document is a partial listing of passenger data from the Titanic.  It includes details such as passenger ID, survival status, passenger class, name, gender, age, number of siblings and spouses aboard, number of parents and children aboard, ticket number, fare, cabin number, and port of embarkation.  This data could be used for analysis of passenger demographics and survival rates. \\n'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Generate a bar chart to show sales data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mFile Selection Expert\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCompare the user query with file content summaries and return a list of relevant file names that can be used for the user query. The file summaries are: {'FAhad Resume.pdf': 'This is a resume for Fahad Ul Rehman, an AI engineer with 5 years of experience. He specializes in generative AI projects and prompt engineering, with proficiency in FastAPI, MongoDB, and cloud-based AI services. He has worked on various AI applications, including procurement and financial data analysis, computer vision, and natural language processing, demonstrating his skills in machine learning, data analytics, and model development. The document showcases his expertise and career history, highlighting his capabilities and achievements in the field of artificial intelligence. \\n', 'data.csv': 'This document is a partial listing of passenger data from the Titanic.  It includes details such as passenger ID, survival status, passenger class, name, gender, age, number of siblings and spouses aboard, number of parents and children aboard, ticket number, fare, cabin number, and port of embarkation.  This data could be used for analysis of passenger demographics and survival rates. \\n'}\n",
      " The user query is: Generate a bar chart to show sales data\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mFile Selection Expert\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "['data.csv']\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = {\n",
    "    'user_query': user_query,\n",
    "    'file_summaries': file_summary_dict\n",
    "}\n",
    "\n",
    "files_to_use = file_selection_crew.kickoff(inputs=inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['data.csv']\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_use.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import CodeInterpreterTool\n",
    "run_codes = CodeInterpreterTool(unsafe_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, LLM\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "data_analyst_agent = Agent(\n",
    "    llm= llm,\n",
    "    role=\"Data Analyst\",\n",
    "    goal=\"Process user queries and file paths by writing and executing Python code.\",\n",
    "    backstory=\"You are an expert data analyst who can do detailed data analysis and create visualisations. You load the datasets and then do the analysis and visualization creation by writing and executing python code.\",\n",
    "    allow_code_execution=True,\n",
    "    tools= [run_codes]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task\n",
    "\n",
    "user_query = \"create a pairplot of the data.\"\n",
    "file_path = \"data.csv\"\n",
    "\n",
    "\n",
    "dataset_data_analysis_task = Task(\n",
    "    description=\"{user_query} The data is located at {file_path}.\",\n",
    "    agent=data_analyst_agent,\n",
    "    expected_output=\"answer in markdown\"\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_analyst_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_data_analysis_crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m----> 2\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[\u001b[43mdata_analyst_agent\u001b[49m],\n\u001b[0;32m      3\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[dataset_data_analysis_task],\n\u001b[0;32m      4\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m dataset_data_analysis_crew\u001b[38;5;241m.\u001b[39mkickoff()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_analyst_agent' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_data_analysis_crew = Crew(\n",
    "    agents=[data_analyst_agent],\n",
    "    tasks=[dataset_data_analysis_task],\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "result = dataset_data_analysis_crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'user_query': user_query, \"file_path\":}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation_agent = Agent(\n",
    "    llm=llm,\n",
    "    role=\"Data Preparation Specialist\",\n",
    "    goal=\"Interpret user queries and extract relevant data from provided file content.\",\n",
    "    backstory=\"An expert in data extraction and preparation, skilled at understanding user requirements and processing raw data accordingly.\",\n",
    "    allow_code_execution=False  # This agent doesn't execute code\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis_agent = Agent(\n",
    "    llm=llm,\n",
    "    role=\"Data Analyst\",\n",
    "    goal=\"Perform data analysis and create visualizations based on processed data.\",\n",
    "    backstory=\"A proficient data analyst capable of executing Python code to analyze data and generate insightful visualizations.\",\n",
    "    allow_code_execution=True,\n",
    "    tools=[CodeInterpreterTool(unsafe_mode=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation_task = Task(\n",
    "    description=\"Interpret the user query and extract the necessary data from the provided file content. The user query is : {user_query}\\n The file content is: {file_content}\",\n",
    "    agent=data_preparation_agent,\n",
    "    expected_output=\"Processed data ready for analysis.\"\n",
    ")\n",
    "\n",
    "data_analysis_task = Task(\n",
    "    description=\"Analyze the processed data and generate visualizations as per the user query.\",\n",
    "    agent=data_analysis_agent,\n",
    "    expected_output=\"Data analysis report with visualizations.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis_crew = Crew(\n",
    "    agents=[data_preparation_agent, data_analysis_agent],\n",
    "    tasks=[data_preparation_task, data_analysis_task],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Generate a bar chart to show the experience in years at each company\"\n",
    "# file_content = \"\"\"\n",
    "# Date,Product,Sales\n",
    "# 2025-01-01,Product A,100\n",
    "# 2025-01-01,Product B,150\n",
    "# 2025-01-02,Product A,200\n",
    "# 2025-01-02,Product B,250\n",
    "# \"\"\"\n",
    "\n",
    "file_content = read_file_content(\"FAhad Resume.pdf\")\n",
    "\n",
    "inputs = {\n",
    "    'user_query': user_query,\n",
    "    'file_content': file_content\n",
    "}\n",
    "\n",
    "result = data_analysis_crew.kickoff({\n",
    "    'user_query': user_query,\n",
    "    'file_content': file_content\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualizer_agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
